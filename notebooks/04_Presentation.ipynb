{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1VFpv5DFQPpyWfVxIvEIrvl7f5s21oVnw","authorship_tag":"ABX9TyNnpi2qC2L5ScAhduD1V/vB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","# é‡æ–°æŒ‚è½½ Google Drive\n","drive.mount('/content/drive', force_remount=True)\n","print(\"âœ… Google Drive å·²é‡æ–°æŒ‚è½½ã€‚\")\n","\n","# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n","MODEL_PATH = '/content/drive/MyDrive/CA6001_DEPRESSION_PROJECT/models/final_student_nn_8_4_distilled.h5'\n","if os.path.exists(MODEL_PATH):\n","    print(f\"âœ… æ¨¡å‹æ–‡ä»¶ç¡®è®¤å­˜åœ¨äº: {MODEL_PATH}\")\n","else:\n","    print(f\"âŒ è­¦å‘Šï¼šæ¨¡å‹æ–‡ä»¶ä»æœªæ‰¾åˆ°ã€‚è¯·æ‰‹åŠ¨æ£€æŸ¥è¯¥è·¯å¾„ä¸‹çš„æ–‡ä»¶ã€‚\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Az1JCf5f6MR","executionInfo":{"status":"ok","timestamp":1765559601831,"user_tz":-480,"elapsed":3913,"user":{"displayName":"haocheng wang","userId":"12706383621310471916"}},"outputId":"de18f917-e3b0-447e-fff8-23d53b9934fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","âœ… Google Drive å·²é‡æ–°æŒ‚è½½ã€‚\n","âœ… æ¨¡å‹æ–‡ä»¶ç¡®è®¤å­˜åœ¨äº: /content/drive/MyDrive/CA6001_DEPRESSION_PROJECT/models/final_student_nn_8_4_distilled.h5\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","import joblib\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import SMOTE\n","\n","# --- è·¯å¾„å¸¸é‡ ---\n","PROJECT_ROOT = '/content/drive/MyDrive/CA6001_DEPRESSION_PROJECT/'\n","BASE_MODEL_DIR = os.path.join(PROJECT_ROOT, 'models')\n","FINAL_ENCODED_DATA_PATH = os.path.join(PROJECT_ROOT, 'data/processed/final_encoded_data.csv')\n","SCALER_PATH = os.path.join(BASE_MODEL_DIR, 'behav_scaler.pkl')\n","\n","# ======================================================================\n","# æ­¥éª¤ 1: æ¢å¤æ•°æ®å¤„ç†æµç¨‹ (ç²¾ç¡®åŒ¹é…æ‚¨çš„ä»£ç )\n","# ======================================================================\n","print(\"--- æ­¥éª¤ 1: æ¢å¤ Standard Scaler åŠæ•°æ®å˜é‡ ---\")\n","try:\n","    # 1. åŠ è½½å·²ç¼–ç çš„æ•°æ®\n","    df = pd.read_csv(FINAL_ENCODED_DATA_PATH)\n","\n","    # 2. æ¸…æ´—åˆ—å (åŒ¹é…æ‚¨çš„ç¼–ç å’ŒéªŒè¯ä»£ç )\n","    df.columns = df.columns.str.lower()\n","    features_to_scale = ['age', 'cgpa', 'studyhours', 'acadpressure', 'studysat', 'finstress']\n","\n","    # 3. æ‹†åˆ†æ•°æ®\n","    X = df.drop(columns=['depression']).copy()\n","    y = df['depression'].copy()\n","    features_to_scale = [col for col in features_to_scale if col in X.columns]\n","\n","    X_train_val, _, y_train_val, _ = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n","    val_size_relative = 0.15 / 0.85\n","    X_train, _, y_train, _ = train_test_split(X_train_val, y_train_val, test_size=val_size_relative, random_state=42, stratify=y_train_val)\n","\n","    # 4. æ‹Ÿåˆ Standard Scaler (åœ¨ X_train ä¸Š fit)\n","    scaler = StandardScaler()\n","    X_train.loc[:, features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n","\n","    # 5. SMOTE (åœ¨å·²ç¼©æ”¾çš„ X_train ä¸Šæ‰§è¡Œ)\n","    smote = SMOTE(random_state=42)\n","    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n","    X_train_full = X_train_resampled # æœ€ç»ˆçš„æ¨¡å‹è®­ç»ƒè¾“å…¥æ•°æ®\n","\n","    # 6. å®šä¹‰ Gradio æ‰€éœ€çš„ behav_scaler\n","    # âš ï¸ å…³é”®ä¿®å¤ï¼šä¸ºäº†éƒ¨ç½²ï¼Œæˆ‘ä»¬åªéœ€è¦ç¼©æ”¾è¡Œä¸ºç‰¹å¾ã€‚æˆ‘ä»¬ä½¿ç”¨åŸå§‹ scaler çš„å‚æ•°\n","    # æ¥é‡æ–°æ„å»ºä¸€ä¸ªåªé’ˆå¯¹è¡Œä¸ºç‰¹å¾çš„ scalerï¼Œæˆ–è€…ç›´æ¥ç”¨åŸå§‹ scaler çš„ transform æ–¹æ³•ã€‚\n","\n","    # ç”±äºæ‚¨çš„æ¨¡å‹åªä½¿ç”¨äº†å‰ 10 ä¸ªè¡Œä¸ºç‰¹å¾\n","    BEHAV_FEATURES = X_train_full.columns[:10].tolist()\n","\n","    # é‡æ–°æ‹Ÿåˆä¸€ä¸ªåªé’ˆå¯¹è¡Œä¸ºç‰¹å¾çš„ scalerï¼Œä½†å‚æ•°æºäºåŸå§‹ X_train çš„fitç»“æœ\n","    # ç®€ä¾¿èµ·è§ï¼Œæˆ‘ä»¬ç›´æ¥å°†åŸå§‹ scaler ç”¨äºéƒ¨ç½²ï¼Œå¹¶åªæŠ½å–å…¶ transform ç»“æœ\n","\n","    # æ‹Ÿåˆå¹¶ä¿å­˜ Gradio éƒ¨ç½²æ‰€éœ€çš„ behav_scaler (å®ƒå¿…é¡»æ˜¯é’ˆå¯¹ BEHAV_FEATURES çš„)\n","    # æˆ‘ä»¬éœ€è¦ç”¨åŸå§‹ X_train çš„è¡Œä¸ºç‰¹å¾æ¥fitä¸€ä¸ªæ–° scalerï¼Œä¿è¯å®ƒçš„ fit åªçœ‹è¡Œä¸ºç‰¹å¾\n","\n","    # æå–åŸå§‹ X_train ä¸­çš„è¡Œä¸ºç‰¹å¾ï¼ˆæœªç¼©æ”¾ï¼‰\n","    X_train_behav_raw = df.loc[X_train.index, BEHAV_FEATURES]\n","\n","    behav_scaler = StandardScaler()\n","    behav_scaler.fit(X_train_behav_raw) # åœ¨åŸå§‹ X_train çš„è¡Œä¸ºç‰¹å¾å­é›†ä¸Šæ‹Ÿåˆ\n","\n","    joblib.dump(behav_scaler, SCALER_PATH)\n","    print(f\"âœ… StandardScaler (behav_scaler) å·²æ‹Ÿåˆå¹¶ä¿å­˜åˆ°: {SCALER_PATH}\")\n","\n","except Exception as e:\n","    print(f\"âŒ ä¸¥é‡é”™è¯¯ï¼šStandardScaler æ¢å¤å¤±è´¥ã€‚é”™è¯¯: {e}\")\n","    exit()\n","\n","# ======================================================================\n","# æ­¥éª¤ 2: éªŒè¯æ£€æŸ¥ (ä½¿ç”¨å·²ç¼©æ”¾çš„ X_train_full)\n","# ======================================================================\n","print(\"\\n--- æ­¥éª¤ 2: Standard Scaler éªŒè¯æ£€æŸ¥ ---\")\n","\n","# 1. æ£€æŸ¥ Gradio ä½¿ç”¨çš„ scaler æ‹Ÿåˆçš„å‡å€¼å’Œæ ‡å‡†å·®\n","print(f\"æ‹Ÿåˆçš„ç‰¹å¾å‡å€¼ (å‰3ä¸ª): {behav_scaler.mean_[:3]}\")\n","print(f\"æ‹Ÿåˆçš„ç‰¹å¾æ ‡å‡†å·® (å‰3ä¸ª): {behav_scaler.scale_[:3]}\")\n","\n","# 2. éªŒè¯ X_train_full (å·² SMOTE å’Œ Scaled) ä¸Šçš„å¹³å‡å€¼\n","# æˆ‘ä»¬åªæ£€æŸ¥ BEHAV_FEATURES çš„å¹³å‡å€¼ï¼Œå®ƒä»¬åº”è¯¥æ¥è¿‘ 0ã€‚\n","X_train_behav_scaled = X_train_full[BEHAV_FEATURES]\n","\n","# è®¡ç®—å·²ç¼©æ”¾è®­ç»ƒé›† (X_train_full) çš„å‡å€¼\n","mean_check = X_train_behav_scaled.mean()\n","std_check = X_train_behav_scaled.std()\n","\n","print(\"\\n--- å…³é”®æ£€æŸ¥ï¼šå·²ç¼©æ”¾è®­ç»ƒé›† (X_train_full) çš„ç»Ÿè®¡ä¿¡æ¯ ---\")\n","print(\"ç‰¹å¾å\\t\\tå¹³å‡å€¼ (åº”æ¥è¿‘ 0)\\tæ ‡å‡†å·® (åº”æ¥è¿‘ 1)\")\n","print(\"-\" * 55)\n","for feature in BEHAV_FEATURES[:5]:\n","    # æ‰“å°å‰ 5 ä¸ªè¡Œä¸ºç‰¹å¾\n","    print(f\"{feature:<15}\\t{mean_check[feature]:.6f}\\t\\t{std_check[feature]:.6f}\")\n","\n","print(\"\\nğŸ‰ Standard Scaler æ¢å¤æˆåŠŸä¸”é€šè¿‡éªŒè¯æ£€æŸ¥ï¼\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7N9lftmPyjER","executionInfo":{"status":"ok","timestamp":1765563925501,"user_tz":-480,"elapsed":2210,"user":{"displayName":"haocheng wang","userId":"12706383621310471916"}},"outputId":"ec701111-48b9-465f-82fa-8a5df94a397b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["--- æ­¥éª¤ 1: æ¢å¤ Standard Scaler åŠæ•°æ®å˜é‡ ---\n","âœ… StandardScaler (behav_scaler) å·²æ‹Ÿåˆå¹¶ä¿å­˜åˆ°: /content/drive/MyDrive/CA6001_DEPRESSION_PROJECT/models/behav_scaler.pkl\n","\n","--- æ­¥éª¤ 2: Standard Scaler éªŒè¯æ£€æŸ¥ ---\n","æ‹Ÿåˆçš„ç‰¹å¾å‡å€¼ (å‰3ä¸ª): [ 0.55615329 25.81119992  3.14314991]\n","æ‹Ÿåˆçš„ç‰¹å¾æ ‡å‡†å·® (å‰3ä¸ª): [0.4968368 4.9120624 1.3808158]\n","\n","--- å…³é”®æ£€æŸ¥ï¼šå·²ç¼©æ”¾è®­ç»ƒé›† (X_train_full) çš„ç»Ÿè®¡ä¿¡æ¯ ---\n","ç‰¹å¾å\t\tå¹³å‡å€¼ (åº”æ¥è¿‘ 0)\tæ ‡å‡†å·® (åº”æ¥è¿‘ 1)\n","-------------------------------------------------------\n","gender         \t0.541386\t\t0.498295\n","age            \t0.035857\t\t1.003476\n","acadpressure   \t-0.086886\t\t1.001524\n","cgpa           \t-0.004646\t\t0.997919\n","studysat       \t0.028629\t\t0.992038\n","\n","ğŸ‰ Standard Scaler æ¢å¤æˆåŠŸä¸”é€šè¿‡éªŒè¯æ£€æŸ¥ï¼\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","from sklearn.model_selection import train_test_split\n","\n","# --- è·¯å¾„å¸¸é‡ ---\n","PROJECT_ROOT = '/content/drive/MyDrive/CA6001_DEPRESSION_PROJECT/'\n","FINAL_ENCODED_DATA_PATH = os.path.join(PROJECT_ROOT, 'data/processed/final_encoded_data.csv')\n","FEATURES_LIST_PATH = os.path.join(PROJECT_ROOT, 'data/processed/final_feature_list.txt')\n","\n","# ======================================================================\n","# æ­¥éª¤ 1: æ¢å¤æ•°æ®å¹¶å¯¼å‡ºç‰¹å¾åˆ—è¡¨\n","# ======================================================================\n","print(\"--- å¯åŠ¨ç‰¹å¾åˆ—è¡¨å¯¼å‡º ---\")\n","try:\n","    # æ¢å¤æ•°æ®å¤„ç†æµç¨‹\n","    df = pd.read_csv(FINAL_ENCODED_DATA_PATH)\n","\n","    # âš ï¸ å…³é”®ç‚¹ï¼šæ‰§è¡Œå¿…è¦çš„åˆ—åç»Ÿä¸€ (ç¡®ä¿æ˜¯å°å†™)\n","    df.columns = df.columns.str.lower()\n","\n","    # æ‹†åˆ†æ•°æ®ä»¥è·å¾— X_train\n","    X = df.drop(columns=['depression']).copy()\n","    y = df['depression'].copy()\n","\n","    X_train_val, _, y_train_val, _ = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n","    val_size_relative = 0.15 / 0.85\n","    X_train, _, y_train, _ = train_test_split(X_train_val, y_train_val, test_size=val_size_relative, random_state=42, stratify=y_train_val)\n","\n","    # è·å–å®Œæ•´çš„åˆ—ååˆ—è¡¨\n","    all_columns = X_train.columns.tolist()\n","\n","    # ç”¨æˆ·çš„ç²¾ç¡® BEHAVIORAL_FEATURES åˆ—è¡¨ (åŸå§‹æ ¼å¼ï¼Œä»…ç”¨äºå®šä½)\n","    BEHAVIORAL_FEATURES_RAW = [\n","        'StudyHours',\n","        'DietHabit_Healthy', 'DietHabit_Moderate', 'DietHabit_Others', 'DietHabit_Unhealthy',\n","        'SleepDur_5-6 hours', 'SleepDur_7-8 hours', 'SleepDur_Less than 5 hours',\n","        'SleepDur_More than 8 hours', 'SleepDur_Others'\n","    ]\n","\n","    # è½¬æ¢ä¸ºå°å†™\n","    BEHAVIORAL_FEATURES_LOWER = [f.lower() for f in BEHAVIORAL_FEATURES_RAW]\n","\n","    # æŸ¥æ‰¾è¿™äº›ç‰¹å¾åœ¨ X_train ä¸­çš„ç¡®åˆ‡åˆ—å\n","    final_behavioral_features = []\n","\n","    for required_feature in BEHAVIORAL_FEATURES_LOWER:\n","        # å°è¯•ç²¾ç¡®åŒ¹é…ï¼ˆä¿ç•™ç©ºæ ¼å’Œè¿å­—ç¬¦ï¼‰\n","        matched_col = next((col for col in all_columns if col == required_feature), None)\n","\n","        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ä¸‹åˆ’çº¿ç‰ˆæœ¬\n","        if matched_col is None:\n","            normalized_req = required_feature.replace('-', '_').replace(' ', '_')\n","            matched_col = next((col for col in all_columns if col.replace('-', '_').replace(' ', '_') == normalized_req), None)\n","\n","        if matched_col:\n","            final_behavioral_features.append(matched_col)\n","        else:\n","            print(f\"âŒ è­¦å‘Šï¼šæœªæ‰¾åˆ°ç‰¹å¾ '{required_feature}' çš„åŒ¹é…åˆ—ã€‚\")\n","\n","    # å¯¼å‡ºæœ€ç»ˆçš„ 10 ä¸ªç‰¹å¾åˆ—è¡¨\n","    if len(final_behavioral_features) == 10:\n","        with open(FEATURES_LIST_PATH, 'w') as f:\n","            for feature in final_behavioral_features:\n","                f.write(feature + '\\n')\n","\n","        print(f\"\\nâœ… æˆåŠŸï¼æœ€ç»ˆ BEHAVIORAL FEATURES å·²å¯¼å‡ºåˆ°: {FEATURES_LIST_PATH}\")\n","        print(\"è¯·è¿è¡Œä¸‹ä¸€æ­¥çš„ä»£ç å—ã€‚\")\n","\n","    else:\n","        print(f\"âŒ é”™è¯¯ï¼šåªæ‰¾åˆ° {len(final_behavioral_features)} ä¸ªç‰¹å¾ã€‚æ— æ³•ç»§ç»­ã€‚\")\n","\n","except Exception as e:\n","    print(f\"\\nâŒ å¯¼å‡ºè„šæœ¬å¤±è´¥ã€‚é”™è¯¯: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0SAcUEqa3nsK","executionInfo":{"status":"ok","timestamp":1765565228706,"user_tz":-480,"elapsed":4611,"user":{"displayName":"haocheng wang","userId":"12706383621310471916"}},"outputId":"3ea5193c-d9c4-4bcb-eb32-8875a9b6a425"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--- å¯åŠ¨ç‰¹å¾åˆ—è¡¨å¯¼å‡º ---\n","\n","âœ… æˆåŠŸï¼æœ€ç»ˆ BEHAVIORAL FEATURES å·²å¯¼å‡ºåˆ°: /content/drive/MyDrive/CA6001_DEPRESSION_PROJECT/data/processed/final_feature_list.txt\n","è¯·è¿è¡Œä¸‹ä¸€æ­¥çš„ä»£ç å—ã€‚\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import SMOTE\n","import joblib\n","\n","# --- è·¯å¾„å¸¸é‡ ---\n","PROJECT_ROOT = '/content/drive/MyDrive/CA6001_DEPRESSION_PROJECT/'\n","FINAL_ENCODED_DATA_PATH = os.path.join(PROJECT_ROOT, 'data/processed/final_encoded_data.csv')\n","FEATURES_LIST_PATH = os.path.join(PROJECT_ROOT, 'data/processed/final_feature_list.txt')\n","SCALER_PATH = os.path.join(PROJECT_ROOT, 'models/behav_scaler.pkl') # ç”¨äºä¿å­˜æ‹Ÿåˆåçš„ scaler\n","\n","# --- è¾…åŠ©å‡½æ•° ---\n","def load_features(path):\n","    with open(path, 'r') as f:\n","        features = [line.strip() for line in f if line.strip()]\n","    return features\n","\n","# ç”¨æˆ·çš„ç²¾ç¡® BEHAVIORAL_FEATURES (å°å†™ï¼Œä»æ–‡ä»¶è¯»å–)\n","try:\n","    BEHAV_FEATURES_FILE = load_features(FEATURES_LIST_PATH)\n","except:\n","    print(\"âŒ é”™è¯¯ï¼šæ— æ³•åŠ è½½ final_feature_list.txt æ–‡ä»¶ã€‚è¯·ç¡®ä¿è¯¥æ–‡ä»¶å­˜åœ¨ã€‚\")\n","    exit()\n","\n","# å‡è®¾æ‰€æœ‰éœ€è¦ç¼©æ”¾çš„è¿ç»­ç‰¹å¾ï¼ˆå°å†™ï¼‰\n","# æ ¹æ®æ‚¨ä¹‹å‰çš„ df.info() å’Œé”™è¯¯ä¿¡æ¯ï¼Œè¿™äº›éƒ½æ˜¯è¿ç»­çš„ï¼š\n","ALL_CONTINUOUS_FEATURES = ['age', 'cgpa', 'studyhours', 'acadpressure', 'studysat', 'finstress']\n","\n","# ======================================================================\n","# æ­¥éª¤ 1: æ¢å¤æ•°æ®å¤„ç†æµç¨‹å¹¶æ‹Ÿåˆ Scaler\n","# ======================================================================\n","print(\"--- æ­¥éª¤ 1: æ¢å¤æ•°æ®å¹¶æ‹Ÿåˆ StandardScaler ---\")\n","try:\n","    # æ¢å¤æ•°æ®å¤„ç†æµç¨‹\n","    df = pd.read_csv(FINAL_ENCODED_DATA_PATH)\n","    df.columns = df.columns.str.lower()\n","\n","    X = df.drop(columns=['depression']).copy()\n","    y = df['depression'].copy()\n","\n","    # æ‰§è¡Œæ‹†åˆ†\n","    X_train_val, _, y_train_val, _ = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n","    val_size_relative = 0.15 / 0.85\n","    X_train, _, y_train, _ = train_test_split(X_train_val, y_train_val, test_size=val_size_relative, random_state=42, stratify=y_train_val)\n","\n","    # æ‰¾å‡º BEHAV_FEATURES_FILE ä¸­å“ªäº›æ˜¯è¿ç»­ç‰¹å¾ (å³å¯ç¼©æ”¾éƒ¨åˆ†)\n","    SCALEABLE_BEHAV_FEATURES = [f for f in BEHAV_FEATURES_FILE if f in ALL_CONTINUOUS_FEATURES]\n","\n","    if not SCALEABLE_BEHAV_FEATURES:\n","        print(\"âŒ é”™è¯¯ï¼šBEHAV_FEATURES_FILE ä¸­æ²¡æœ‰æ‰¾åˆ°å¯ç¼©æ”¾çš„è¿ç»­ç‰¹å¾ã€‚æ— æ³•æ‹Ÿåˆ Scalerã€‚\")\n","        exit()\n","\n","    # æ‹Ÿåˆ behav_scaler (åªåœ¨ X_train çš„å¯ç¼©æ”¾å­é›†ä¸Š fit)\n","    X_train_for_behav_fit = X_train[SCALEABLE_BEHAV_FEATURES]\n","\n","    behav_scaler = StandardScaler()\n","    behav_scaler.fit(X_train_for_behav_fit)\n","\n","    joblib.dump(behav_scaler, SCALER_PATH)\n","    print(f\"âœ… StandardScaler (behav_scaler) å·²åœ¨ {SCALEABLE_BEHAV_FEATURES} ä¸Šæ‹Ÿåˆå¹¶ä¿å­˜ã€‚\")\n","    print(f\"æ‹Ÿåˆç‰¹å¾æ•°é‡: {len(SCALEABLE_BEHAV_FEATURES)}\")\n","\n","except Exception as e:\n","    print(f\"âŒ æ­¥éª¤ 1 å¤±è´¥ï¼šStandardScaler æ¢å¤å¤±è´¥ã€‚é”™è¯¯: {e}\")\n","    exit()\n","\n","# ======================================================================\n","# æ­¥éª¤ 2: éªŒè¯ Scaler çš„æ‹Ÿåˆç»“æœå’Œé¢„æµ‹è¾“å…¥æ˜¯å¦åŒ¹é…\n","# ======================================================================\n","print(\"\\n--- æ­¥éª¤ 2: éªŒè¯ Scaler æ‹Ÿåˆç»“æœ ---\")\n","\n","# æ£€æŸ¥ Scaler æœŸæœ›çš„ç‰¹å¾æ•°é‡å’Œåç§°æ˜¯å¦åŒ¹é…\n","scaler_fit_features = X_train_for_behav_fit.columns.tolist()\n","\n","if scaler_fit_features != SCALEABLE_BEHAV_FEATURES:\n","    print(\"âŒ éªŒè¯å¤±è´¥ï¼šScaler æ‹Ÿåˆç‰¹å¾é¡ºåºæˆ–åç§°ä¸åŒ¹é…ï¼\")\n","    print(f\"Scaler æ‹Ÿåˆçš„ç‰¹å¾: {scaler_fit_features}\")\n","    print(f\"ä»£ç é¢„æœŸçš„ç‰¹å¾: {SCALEABLE_BEHAV_FEATURES}\")\n","    exit()\n","else:\n","    print(\"âœ… éªŒè¯é€šè¿‡ï¼šScaler æ‹Ÿåˆç‰¹å¾åˆ—è¡¨ä¸é¢„æœŸä¸€è‡´ã€‚\")\n","    print(f\"Scaler æ‹Ÿåˆçš„ {len(scaler_fit_features)} ä¸ªç‰¹å¾: {scaler_fit_features}\")\n","\n","print(\"\\n--- è¯Šæ–­æˆåŠŸ ---\")\n","print(\"å¦‚æœä»¥ä¸Šæ­¥éª¤é€šè¿‡ï¼Œåˆ™ Scaler æ‹Ÿåˆå’Œ Gradio é¢„æµ‹æ‰€éœ€çš„æ•°æ®åˆ‡ç‰‡åº”è¯¥åŒ¹é…ã€‚\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51J-3VsN5WSJ","executionInfo":{"status":"ok","timestamp":1765565473850,"user_tz":-480,"elapsed":463,"user":{"displayName":"haocheng wang","userId":"12706383621310471916"}},"outputId":"9f0c1fa0-d750-4576-98a3-aead2629f076"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--- æ­¥éª¤ 1: æ¢å¤æ•°æ®å¹¶æ‹Ÿåˆ StandardScaler ---\n","âœ… StandardScaler (behav_scaler) å·²åœ¨ ['studyhours'] ä¸Šæ‹Ÿåˆå¹¶ä¿å­˜ã€‚\n","æ‹Ÿåˆç‰¹å¾æ•°é‡: 1\n","\n","--- æ­¥éª¤ 2: éªŒè¯ Scaler æ‹Ÿåˆç»“æœ ---\n","âœ… éªŒè¯é€šè¿‡ï¼šScaler æ‹Ÿåˆç‰¹å¾åˆ—è¡¨ä¸é¢„æœŸä¸€è‡´ã€‚\n","Scaler æ‹Ÿåˆçš„ 1 ä¸ªç‰¹å¾: ['studyhours']\n","\n","--- è¯Šæ–­æˆåŠŸ ---\n","å¦‚æœä»¥ä¸Šæ­¥éª¤é€šè¿‡ï¼Œåˆ™ Scaler æ‹Ÿåˆå’Œ Gradio é¢„æµ‹æ‰€éœ€çš„æ•°æ®åˆ‡ç‰‡åº”è¯¥åŒ¹é…ã€‚\n"]}]},{"cell_type":"code","source":["import gradio as gr\n","import pandas as pd\n","import numpy as np\n","import os\n","import joblib\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import SMOTE\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","\n","# --- è·¯å¾„å¸¸é‡ ---\n","PROJECT_ROOT = '/content/drive/MyDrive/CA6001_DEPRESSION_PROJECT/'\n","BASE_MODEL_DIR = os.path.join(PROJECT_ROOT, 'models')\n","FINAL_ENCODED_DATA_PATH = os.path.join(PROJECT_ROOT, 'data/processed/final_encoded_data.csv')\n","FEATURES_LIST_PATH = os.path.join(PROJECT_ROOT, 'data/processed/final_feature_list.txt')\n","\n","FINAL_STUDENT_MODEL_PATH = os.path.join(BASE_MODEL_DIR, 'nn_student_distilled.keras')\n","SCALER_PATH = os.path.join(BASE_MODEL_DIR, 'behav_scaler.pkl')\n","\n","# ----------------------------------------------------------------------\n","# æ ¸å¿ƒè¾…åŠ©å‡½æ•°å’Œå˜é‡\n","# ----------------------------------------------------------------------\n","\n","def load_features(path):\n","    \"\"\"ä»æ–‡ä»¶ä¸­è¯»å–å‡†ç¡®çš„ç‰¹å¾åˆ—è¡¨ï¼ˆå°å†™ï¼‰ã€‚\"\"\"\n","    with open(path, 'r') as f:\n","        features = [line.strip() for line in f if line.strip()]\n","    return features\n","\n","def get_one_hot_col_name(category, value, feature_list):\n","    \"\"\"æ ¹æ®Gradioçš„åŸå§‹å€¼ï¼ŒæŸ¥æ‰¾ç²¾ç¡®åŒ¹é…çš„å°å†™åˆ—åã€‚\"\"\"\n","    normalized_value = value.lower()\n","    normalized_category = category.lower()\n","    target_name = f'{normalized_category}_{normalized_value}'\n","\n","    match = next((col for col in feature_list if col == target_name), None)\n","\n","    target_name_underscore = target_name.replace(' ', '_').replace('-', '_')\n","    if match is None:\n","        match = next((col for col in feature_list if col == target_name_underscore), None)\n","\n","    return match\n","\n","# ----------------------------------------------------------------------\n","# æ­¥éª¤ 1: æ¢å¤ Gradio è¿è¡Œç¯å¢ƒ\n","# ----------------------------------------------------------------------\n","print(\"--- æ­¥éª¤ 1: æ¢å¤ Gradio è¿è¡Œç¯å¢ƒ ---\")\n","try:\n","    tf.keras.backend.clear_session()\n","\n","    # ä»æ–‡ä»¶åŠ è½½ BEHAV_FEATURES (é»„é‡‘æ ‡å‡†)\n","    BEHAV_FEATURES_FILE = load_features(FEATURES_LIST_PATH)\n","    SCALEABLE_BEHAV_FEATURES = ['studyhours'] # ç¡®è®¤åªæœ‰è¿™ä¸€ä¸ªéœ€è¦ç¼©æ”¾\n","\n","    # æ¢å¤æ•°æ®æµç¨‹ä»¥è·å– ALL_TRAINING_FEATURES (ç”¨äºæ„é€ å…¨é›¶å‘é‡)\n","    df = pd.read_csv(FINAL_ENCODED_DATA_PATH)\n","    df.columns = df.columns.str.lower()\n","    X = df.drop(columns=['depression']).copy()\n","    y = df['depression'].copy()\n","\n","    X_train_val, _, y_train_val, _ = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n","    val_size_relative = 0.15 / 0.85\n","    X_train, _, y_train, _ = train_test_split(X_train_val, y_train_val, test_size=val_size_relative, random_state=42, stratify=y_train_val)\n","\n","    smote = SMOTE(random_state=42)\n","    X_train_full, _ = smote.fit_resample(X_train, y_train)\n","    ALL_TRAINING_FEATURES = X_train_full.columns.tolist()\n","\n","    # åŠ è½½å·²ä¿å­˜çš„ behav_scaler å’Œæ¨¡å‹\n","    behav_scaler = joblib.load(SCALER_PATH)\n","    model = load_model(FINAL_STUDENT_MODEL_PATH)\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    FEATURE_DEFAULTS_RAW = dict(zip(ALL_TRAINING_FEATURES, [0.0] * len(ALL_TRAINING_FEATURES)))\n","    NON_SCALEABLE_BEHAV_FEATURES = [f for f in BEHAV_FEATURES_FILE if f not in SCALEABLE_BEHAV_FEATURES]\n","\n","    print(\"âœ… Gradio ä¾èµ–æ¢å¤æˆåŠŸã€‚\")\n","\n","except Exception as e:\n","    print(f\"âŒ Gradio ç¯å¢ƒæ¢å¤å¤±è´¥ã€‚é”™è¯¯: {e}\")\n","    exit()\n","\n","# --- Gradio ç•Œé¢é…ç½® ---\n","DIET_HABIT_OPTIONS = ['Healthy', 'Moderate', 'Unhealthy', 'Others']\n","SLEEP_DUR_OPTIONS = ['5-6 hours', '7-8 hours', 'Less than 5 hours', 'More than 8 hours', 'Others']\n","\n","INPUT_FEATURE_NAMES = {\n","    'studyhours': 'å­¦ä¹ æ—¶é•¿ (StudyHours)',\n","    'sleepdur': 'ç¡çœ æ—¶é—´ (SleepDur)',\n","    'diethabit': 'é¥®é£Ÿä¹ æƒ¯ (DietHabit)'\n","}\n","\n","# ----------------------------------------------------------------------\n","# æ ¸å¿ƒé¢„æµ‹é€»è¾‘ (ç‰¹å¾é‡ç»„)\n","# ----------------------------------------------------------------------\n","\n","def predict_depression(study_hours, sleep_dur, diet_habit):\n","    \"\"\" æ¥æ”¶ç”¨æˆ·è¾“å…¥çš„åŸå§‹å€¼ï¼Œæ„é€ ç‰¹å¾å‘é‡å¹¶é¢„æµ‹ã€‚ \"\"\"\n","\n","    input_data_raw = {k: v for k, v in FEATURE_DEFAULTS_RAW.items()}\n","\n","    # 1. æ„é€  X_pred_raw (å…¨ç‰¹å¾å‘é‡)\n","    input_data_raw['studyhours'] = study_hours\n","\n","    matched_sleep_col = get_one_hot_col_name('SleepDur', sleep_dur, ALL_TRAINING_FEATURES)\n","    if matched_sleep_col: input_data_raw[matched_sleep_col] = 1\n","\n","    matched_diet_col = get_one_hot_col_name('DietHabit', diet_habit, ALL_TRAINING_FEATURES)\n","    if matched_diet_col: input_data_raw[matched_diet_col] = 1\n","\n","    X_pred_raw = pd.DataFrame([input_data_raw], columns=ALL_TRAINING_FEATURES)\n","\n","    # 2. ç¼©æ”¾ (ä»… studyhours)\n","    X_pred_scaleable = X_pred_raw[SCALEABLE_BEHAV_FEATURES] # 1 ç»´ (studyhours)\n","    X_pred_scaled = behav_scaler.transform(X_pred_scaleable) # ç¼©æ”¾åçš„ studyhours\n","\n","    # 3. æå–æœªç¼©æ”¾çš„ One-Hot ç‰¹å¾ (9 ç»´)\n","    X_pred_non_scaleable = X_pred_raw[NON_SCALEABLE_BEHAV_FEATURES]\n","\n","    # 4. é‡æ–°ç»„åˆï¼šåˆ›å»ºæœ€ç»ˆçš„ 10 ç»´è¾“å…¥å‘é‡ (é¡ºåºå¿…é¡»åŒ¹é… BEHAV_FEATURES_FILE)\n","\n","    final_data_list = []\n","    # ä½¿ç”¨ä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨ç¼©æ”¾åçš„ studyhoursï¼Œæ–¹ä¾¿æŒ‰é¡ºåºæå–\n","    scaled_dict = dict(zip(SCALEABLE_BEHAV_FEATURES, X_pred_scaled.flatten()))\n","\n","    for feature in BEHAV_FEATURES_FILE:\n","        if feature in SCALEABLE_BEHAV_FEATURES:\n","            # æå–ç¼©æ”¾åçš„ studyhours\n","            final_data_list.append(scaled_dict[feature])\n","        else:\n","            # æå–æœªç¼©æ”¾çš„ One-Hot ç‰¹å¾å€¼\n","            final_data_list.append(X_pred_non_scaleable[feature].iloc[0])\n","\n","    X_final_input = np.array(final_data_list).reshape(1, 10) # æœ€ç»ˆçš„ 1x10 å‘é‡\n","\n","    # 5. é¢„æµ‹\n","    prob = model.predict(X_final_input, verbose=0).flatten()[0]\n","\n","    # 6. ç»“æœè¾“å‡º\n","    if prob >= 0.5:\n","        prediction_text = \"é¢„æµ‹ç»“æœï¼šæŠ‘éƒé£é™©è¾ƒé«˜\"\n","        color = \"red\"\n","    else:\n","        prediction_text = \"é¢„æµ‹ç»“æœï¼šæŠ‘éƒé£é™©è¾ƒä½\"\n","        color = \"green\"\n","\n","    return (\n","        f\"<h2 style='color: {color};'> {prediction_text} </h2>\"\n","        f\"<h3> é¢„æµ‹æ¦‚ç‡: {prob*100:.2f}% </h3>\"\n","        \"<p>æ¨¡å‹å·²éƒ¨ç½²ã€‚è¯·å°è¯•æç«¯è¾“å…¥æ¥éªŒè¯æ¨¡å‹å†³ç­–ã€‚</p>\"\n","    )\n","\n","# ----------------------------------------------------------------------\n","# Gradio ç•Œé¢è®¾è®¡\n","# ----------------------------------------------------------------------\n","iface = gr.Interface(\n","    fn=predict_depression,\n","    inputs=[\n","        gr.Slider(minimum=0, maximum=10, step=0.5, value=3.0, label=INPUT_FEATURE_NAMES['studyhours']),\n","        gr.Radio(choices=SLEEP_DUR_OPTIONS, label=INPUT_FEATURE_NAMES['sleepdur'], value='7-8 hours'),\n","        gr.Radio(choices=DIET_HABIT_OPTIONS, label=INPUT_FEATURE_NAMES['diethabit'], value='Moderate')\n","    ],\n","    outputs=gr.HTML(label=\"é¢„æµ‹ç»“æœ\"),\n","    title=\"ğŸ§  çŸ¥è¯†è’¸é¦ NN æŠ‘éƒé£é™©é¢„æµ‹æ¼”ç¤º (æœ€ç»ˆç‰¹å¾é‡ç»„ç‰ˆ)\",\n","    description=\"æ¨¡å‹åŸºäºæ‚¨çš„ 10 ä¸ªç”Ÿæ´»æ–¹å¼ç‰¹å¾è¿›è¡Œé¢„æµ‹ã€‚å·²è§£å†³ Scaler æ‹Ÿåˆç‰¹å¾ä¸ä¸€è‡´é—®é¢˜ã€‚\",\n","    theme=gr.themes.Soft()\n",")\n","\n","# è¿è¡Œç•Œé¢\n","print(\"\\n--- å¯åŠ¨ Gradio ç•Œé¢ ---\")\n","iface.launch(share=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":719},"id":"lm5ntHEk57ZS","executionInfo":{"status":"ok","timestamp":1765565522916,"user_tz":-480,"elapsed":10008,"user":{"displayName":"haocheng wang","userId":"12706383621310471916"}},"outputId":"5c859559-9a14-4037-8705-9a87ac1c4424"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["--- æ­¥éª¤ 1: æ¢å¤ Gradio è¿è¡Œç¯å¢ƒ ---\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 14 variables. \n","  saveable.load_own_variables(weights_store.get(inner_path))\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Gradio ä¾èµ–æ¢å¤æˆåŠŸã€‚\n","\n","--- å¯åŠ¨ Gradio ç•Œé¢ ---\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://ac5481aa254f262984.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://ac5481aa254f262984.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":5}]}]}